<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <div class="container">

        <section class="elaboration">
            <h2>Introduction: Early years of spark</h2>
            <p>Inspired from Google File System (GFS), MapReduce(MR) and Bigtable. GFS is a fault-tolerant and distributed filesystem.
                Bigtable offered scalable storage of structured data across GFS. MR with functional programming, introduced a new programming paradigm.
            </p>
            <p>MR favours data locality and cluster rack affinity, rather than bringing data to application. Worker node output written
                to a distributed storage, keeping most of I/O local to disc rather than distributing over network. 
            </p>
            <p>
                Hadoop was founded at Yahoo! in April 2006. Hard to manage and administer, cumbersome operational complexity. Secondly,
                batch-processing MR API requires alot of boiler plate setup code, with brittle fault tolerance. Finally, large batches 
                of data jobs with many MR pairs requires many writes of computed intermediate results to local disc, hence major overhead. 
                Also fell short of combination work like ML, streaming or interactive SQL queries.
            </p>
        </section>

        <section class="main-points">
            <p>
                Apache Spark providess in-memory storage for intermediate computations. It has libraries such as MLlib, Spark SQL,
                Structured Streaming and GraphX.
            </p>
            <h2>What is Spark</h2>
            <ul>
                <li>1. Speed</li>
                <li>2. Ease of Use</li>
                <li>3. Modularity</li>
                <li>4. Extensibility</li>
            </ul>

            <p> 
                Spark improved it's speed in several ways. Internal implementations benefits greatly from price and performance
                improvements of CPUs and memory. Underlying Unix implementations exploits effcient multi-threading and 
                parallel processing. <br>
                Spark SQL modelled as DAG, with DAG schedulers and query optimisers. Allows for effcient computation graphs that
                are executed in parallel. <br> 
                Lastly, the physical execution engine, Tungsten, generate compact code for execution.
                Results hence can be retained in memory, reducing I/O overhead.
            </p>

            <p>
                Ease of use: from Resilient Distributed Dataset (RDD) implementations. Provdes a set of transformations and actions
                as operations, providing a simple programming model.
            </p>

            <p>
                Modularity: Applied across many types of workload and support Scala, Java, Python, SQL and R, with many libaries.
            </p>

            <p>
                Extensibility: focus on fast parallel computation engine, decouples storage from compute. Able to use spark
                to read from other data sources and process in memory with `DataFrameReaders` and `DataFrameWriters.`
            </p>

        </section>

        <section class="elaboration">
            <h2>Unified Analytics</h2>
            <p>Unified Engine for Big Data Processing. Seperate from spark's core fault-tolerant engine.</p>
        </section>

        <section class="main-points">
            <h2>Main Points</h2>
            <ul>
                <li>1. Spark SQL</li>
                <li>2. Spark MLlib</li>
                <li>3. Spark Structured Streaming</li>
                <li>4. GraphX</li>
            </ul>

            <p>Spark SQL: combine SQL like queries to query data in Spark Dataframe. Funciton as a pure SQL engine.
                Generated bytecode will be identical, resulting in the same performance.
            </p>

                <pre><code>
// In Scala
// Read data off Amazon S3 bucket into a Spark DataFrame spark.read.json("s3://apache_spark/data/committers.json")
.createOrReplaceTempView("committers")
// Issue a SQL query and return the result as a Spark DataFrame
val results = spark.sql("""SELECT name, org, module, release, num_commits
        FROM committers WHERE module = 'mllib' AND num_commits > 10
        ORDER BY num_commits DESC""")
                </code></pre>

            <p>
                Spark MLlib: API to extract and transform features, build pipelines (for training and evaluating)
                and persist models (for saving and reloading). Also includes common LA, ML and stats operations.
                `spark.mllib` is a RDD-based API in maintenance mode. 'spark.ml' is DataFrame-based. 
            </p>

            <pre><code>
# In Python
from pyspark.ml.classification import LogisticRegression ...
training = spark.read.csv("s3://...")
test = spark.read.csv("s3://...")
    # Load training data
lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8) # Fit the model
    lrModel = lr.fit(training) 
# Predict
    lrModel.transform(test)
    ...
            </code></pre>

            <p>
                Spark Structured Streaming: built ontop of Spark SQL engine and DataFrame-based APIs. Needed for
                combination of big data in both real time and static data. Stream contuniously grow in table, 
                with new roles appended to the end. Treat as structured table and query against it.<br>
                Spark SQL core engine handles all the aspects of fault tolerance and late-data semantics. 
                Removes need for old DStreams (Spark 1.x). Range of streaming data comes from Apache Kafka,
                Kinesis and HDFS-based or cloud storage.
            </p>

            <pre><code>
# In Python
# Read a stream from a local host
from pyspark.sql.functions import explode, split lines = (spark
        .readStream
        .format("socket")
        .option("host", "localhost")
        .option("port", 9999)
        .load())
# Perform transformation
# Split the lines into words
words = lines.select(explode(split(lines.value, " ")).alias("word"))
    # Generate running word count
    word_counts = words.groupBy("word").count()
    # Write out to the stream to Kafka
    query = (word_counts
        .writeStream
        .format("kafka")
        .option("topic", "output"))    
            </code></pre>
            
            <p>GraphX: formulate graph-based computations. Offers standard algos like: PageRank, Connected Components
                and Triangle Counting, traverals. 
            </p>

            <pre><code>
// In Scala
val graph = Graph(vertices, edges)
messages = spark.textFile("hdfs://...")
val graph2 = graph.joinVertices(messages) {
(id, vertex, msg) => ... }
    
            </code></pre>
            
        </section>


        <section class="elaboration">
            <h2>Apache Spark's Distributed Execution</h2>
            <p>Spark consists of a dirver program which is resposible for orchestrating parallel operations in spark clusters using
                SparkSessions.
            </p>

            <p>
                Spark Driver: communicates with cluster manager for resources (CPU, Mem) for Spark excutors (JVMs).
                Transforms Spark operations into DAGs, schedules them and distributes their execution as tasks
                across the Spark executors. Communicates directly with executors.
            </p>

            <p>
                SparkSession: subsumes previous entry points [parkContext, SQLContext, HiveContext, SparkConf, and StreamingContext].
                Maintained backward compatibility. Single session to create JVM runtime parameters, define DataFrames and Datasets, read from data sources, access catalog metadata, and issue Spark SQL queries. Creates a spark sesison per JVM.
            </p>

            <p>
                Cluster Manager: Managing and allocating resources for cluster of nodes. Supports 4 clusters:
                the built-in standalone cluster manager, Apache Hadoop YARN, Apache Mesos, and Kubernetes.
            </p>

            <p>
                Spark executor: runs on each worker node in the cluster. Responsible for executing tasks on the workers.
                In most deployments modes, only a single executor runs per node.
            </p>

            <p>
                Deployment Modes: Support a number of deployment modes, able to run in diff conig and enviro. Cluster manager
                is agnostic to where it runs, can be deployed in many environments like Apache Hadoop YARN and Kubernetes.
            </p>

            <p> 
                Distributed data and partitions: Actual physical data is distributed across storage as partitions either
                in HDFS or cloud storage. Spark treats each partition as a DataFrame in Mem. Each Spark executor is preferably allocated a task that requires it to read the partition closest to it in the network, observing data locality.<br>
                Process only data that is close to them, minimizing network bandwidth. Each executor's core is assigned its own data partition to work on.<br>
                Discuss how to tune and change partitioning configuration for maximum parallelism based on how many cores you have on your executors.
            </p>
<!-- 
        </section>

        <section class="code">
            <h2>Code</h2>
            <pre><code>
// Your code snippet here
def example_function():
    print("Hello, world!")
            </code></pre>
        </section>

        <section class="diagrams">
            <h2>Diagrams</h2>
            <img src="your-diagram.png" alt="Diagram description">
        </section> -->

        <section class="example-questions">
            <h2>Example Questions</h2>
            <p>Q1: What is the purpose of the code above?</p>
            <p>Q2: Explain the key concepts from the main points.</p>
        </section>

        <section class="Not covered">
            <h2>Topics not covered in this book</h2>
            <p>1: low-level Resilient Distributed Datasets APIs</p>
            <p>2: GraphX: Spark's API for graph and graph-parallel computations</p>
            <p>3: Spark's Catalyst opitmiser to implement custom operations</p>
            <p>4: Implemented own catalog</p>
            <p>5: Writing own DataSource V2 data sinks and sources</p>
        </section>
    </div>
</body>
</html>
