<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <div class="container">
        <section class="main-points">
            <h2>Spark SQL</h2>
            <ul>
                <li>Provides the engine upon which the high-level Structured APIs</li>
                <li>Read and Write data in a variety of formats (JSON, Parquet, Arvo)</li>
                <li>Lets you query data using JDBC/ODBC connectors from external business
                    intelligence (BI) data sources such as Tableau, Power BI, Talend, or from RDBMSs 
                    such as MySQL and PostgreSQL</li>
                <li>programmatic interface to interact with structured data stored as tables or 
                    views in a database from a Spark application</li>
                <li>interactive shell to issue SQL queries on your structured data</li>
            </ul>
        </section>


        <section class="elaboration">
            <h2>Using Spark SQL in Spark Applications</h2>
            <p>
                spark.sql("SELECT * FROM myTableName"): place queries inside the function, returns a DF.<br>

                <img src="images/Screenshot 2024-09-02 at 3.35.35 PM.png" alt="Diagram description">
            </p>
        </section>

        <section class="elaboration">
            <h2>SQL Tables and Views</h2>
            <p> 
                Associated with each table in Spark is its relevant metadata, which is information 
                about the table and its data: the schema, description, table name, database name,
                 column names, partitions, physical location where the actual data resides, etc.
                  All of this is stored in a central metastore.<br>

                Spark by default uses the Apache Hive metastore, located at /user/hive/warehouse,
                 to persist all the metadata about your tables. However, you may change the default 
                 location by setting the Spark config variable spark.sql.warehouse.dir to another
                  location, which can be set to a local or external distributed storage.<br>

                When creating Tables, we need to specifiy the database. spark.sql("CREATE DATABASE learn_spark_db")
                spark.sql("USE learn_spark_db"). All tables created afterwards will be attached to this DB.<br>

                Managed table: Spark manages both the metadata and the data in the file store. When DROP TABLE,
                both data and metadata will be deleted.<br>

                Unmanaged table: Spark only manages the metadata, which will be dropped when DROP TABLE occurs.
                Will need to specify path to raw data.<br>

                <img src="images/Screenshot 2024-09-02 at 3.43.02 PM.png" alt="Diagram description">

                <H4>Creating views</H4>
                <img src="images/Screenshot 2024-09-02 at 3.46.18 PM.png" alt="Diagram description">

                    when accessing a global temporary view you must use the prefix global_temp.<view_name>,
                     because Spark creates global temporary views in a global temporary database called global_temp.<br>

                    A temporary view is tied to a single SparkSession within a Spark application. In contrast, a global 
                    temporary view is visible across multiple SparkSessions within a Spark application. Yes, you can
                     create multiple SparkSessions within a single Spark application—this can be handy, for example,
                      in cases where you want to access (and combine) data from two different SparkSessions that don't
                       share the same Hive metastore configurations.<br>
            </p>
        </section>

        <section class="elaboration">
            <h2>Elaboration</h2>
            <p>Detailed explanations and descriptions go here.</p>
        </section>

        <section class="elaboration">
            <h2>MetaData</h2>
            <p>
                Metadata captured in Catalog, a high-level abstraction in Spark SQL for storing metadata.<br>

                <pre><code>
// In Scala/Python
spark.catalog.listDatabases()
spark.catalog.listTables()
spark.catalog.listColumns("us_delay_flights_tbl")
                </code></pre>

                specify a table as LAZY, meaning that it should only be cached when it is first used instead of immediately.<br>
                <pre><code>
-- In SQL
CACHE [LAZY] TABLE <table-name>
UNCACHE TABLE <table-name>
                </code></pre>
            </p>
        </section>

        <section class="code">
            <h2>Code</h2>
            <pre><code>
// Your code snippet here
def example_function():
    print("Hello, world!")
            </code></pre>
        </section>

        <section class="diagrams">
            <h2>Diagrams</h2>
            <img src="your-diagram.png" alt="Diagram description">
        </section>

        <section class="example-questions">
            <h2>Example Questions</h2>
            <p>Q1: What is the purpose of the code above?</p>
            <p>Q2: Explain the key concepts from the main points.</p>
        </section>
    </div>
</body>
</html>
